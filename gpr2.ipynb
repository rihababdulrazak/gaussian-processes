{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import numpy as np \n","import scipy as sp \n","import numpy.random as rand\n","import pandas as pd\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import plotly as py\n","import plotly.graph_objs as go\n","plt.style.use('seaborn-whitegrid')  # some other styles - default, classic, bmh, fast, fivethirtyeight, ggplot, seaborn, seaborn-bright, seaborn-dark, seaborn-dark-palette, seaborn-darkgrid, seaborn-deep, seaborn-notebook, seaborn-ticks, seaborn-whitegrid, tableau-colorblind10\n","plt.rcParams['figure.figsize'] = (20,10)\n","plt.rcParams['xtick.labelsize']=20\n","plt.rcParams['ytick.labelsize']=20\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # This notebook is about Gaussian Process Regression.\n"," ## But first we will look at Gaussian random variables.\n"," ## A single Gaussian random variable is described by its mean value $m$ and variance value $\\sigma^2$:\n"," > ## $p(y) = \\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp^{-\\frac{(y-m)^2}{2 \\sigma^2}} $."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","y = np.linspace(0,10,1000)\n","p = (1/np.sqrt(2*np.pi))*np.exp(-0.5*(y-5)*(y-5))\n","t = go.Scatter(x=y, y=p, name='single Gaussian distribution')\n","fig1 = go.Figure(data=[t])\n","fig1.update_layout(title='Gaussian pdf', xaxis_title='y (value taken by the random variable)', yaxis_title='p(x) (probability density)')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## The above plot shows a Gaussian distribution with $m=5$ and $\\sigma=1$."]},{"cell_type":"markdown","metadata":{},"source":[" ## A multivariate Gaussian distribution is defined over more than one variables.\n"," ## Say, there are two possibly (cor)related variables $y_1$ and $y_2$.\n"," ## Since they may be (cor)related, to evaluate the probability for $y_1$ and $y_2$, we need to consider both of them together (joint random vector ${\\bf{y}} = [y_1, y_2]^T$) as a *joint distribution*:\n"," > ## $ p(y_1,y_2) = p({\\bf y}) = \\frac{1}{\\sqrt{(2 \\pi)^2 |\\Sigma|}} \\exp^{-\\frac{1}{2} ({\\bf y}-{\\bf m})^T \\Sigma^{-1} ({\\bf y}-{\\bf m})} $.\n"," ## In this case, the mean ${\\bf m}$ is a vector ${\\bf m} = [m_1, m_2]^T$ and $\\Sigma$ is a $2 \\times 2$ *covariance matrix*. If the variables are not (cor)related, the covariance matrix $\\Sigma$ will be diagonal and the joint distribution collapses to two individual Gaussian distributions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# create 2D grid\n","x = np.linspace(0,10,100)\n","y = np.linspace(0,10,100)\n","(Y1, Y2) = np.meshgrid(x,y)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# covariance matrix\n","S = np.array([[1,0], [0,2]])\n","Sinv = np.linalg.inv(S)\n","# mean vector\n","m = np.array([[4],[5]])\n","# pdf\n","p = np.zeros(Y1.shape)\n","for i in range(Y1.shape[0]):\n","\tfor j in range(Y1.shape[1]):\n","\t\tYi = np.array([[Y1[i,j]], [Y2[i,j]]])\n","\t\tp[i,j] = (1/np.sqrt(4*np.pi*np.pi*2))*np.exp(-0.5*np.dot((Yi-m).transpose(), np.dot(Sinv,Yi-m)))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","t1 = go.Scatter3d(\n","     x=Y1.flatten(), \n","     y=Y2.flatten(), \n","     z=p.flatten(),\n","     name='multivariate normal',\n","     mode='markers'\n","     )\n","fig2 = go.Figure(data=[t1])\n","fig2.update_layout(\n","     title = 'Multivariate Gaussian',\n","     width=800, \n","     height=800,\n","     scene=dict(\n","         xaxis_title='y1',\n","         yaxis_title='y2',\n","         zaxis_title='p(y1,y2) [pdf]',\n","     ),\n","     margin=dict(l=0, r=0, b=0, t=0)  # tight layout\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## The above plot shows a joint Gaussian distribution with mean ${\\bf m}=[4,5]^T$ and covariance matrix $\\Sigma= \\left( \\begin{array}{cc} 1 & 0 \\\\ 0 & 2 \\end{array} \\right)$.\n"," ## Similarly, joint distribution can be defined for $k$ Gaussian variables (for any finite $k>0$):\n"," > ## $ p(y_1,\\dots,y_k) = p({\\bf y}) = \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}} \\exp^{-\\frac{1}{2} ({\\bf y}-{\\bf m})^T \\Sigma^{-1} ({\\bf y}-{\\bf m})} $.\n"," ## where $\\bf{y}$ is the random vector ${\\bf{y}} = [y_1, \\dots, y_k]^T$."]},{"cell_type":"markdown","metadata":{},"source":[" # Now we come to a random process.\n"," ## A *random process* is a collection of random variables.\n"," ## A *Gaussian process* is a collection of random variables, where any finite number of them are *jointly Gaussian*. A typical example is a time-series, where corresponding to each time point $t$ we have a random variable $X_t$.\n"," ## In the context of Gaussian Process Regression, we wish to model the function\n"," > ## $y=f(\\bf{x})$\n"," > ## ($\\bf{x}$ is bold because it is considered a vector corresponding to multiple features)\n"," ## as a Gaussian process. i.e., given any finite set of points ${\\bf x}_1,{\\bf x}_2,\\cdots,{\\bf x}_k$, we model the joint distribution of ${\\bf y} = [f({\\bf x}_1),f({\\bf x}_2),\\cdots,f({\\bf x}_k)]$ to be a Gaussian distribution with some mean, say $[m({\\bf x}_1),m({\\bf x}_2),\\cdots,m({\\bf x}_k)]$ and covariance matrix $[k({\\bf x}_i, {\\bf x}_j)]$.\n"," ## The Gaussian process $f(\\bf{x})$ is completely specified by its *mean function* $m(\\bf{x})$ and the *covariance function* $k(\\bf{x},\\bf{x}')$:\n"," > ## $f({\\bf{x}}) \\sim \\mathcal{GP}(m({\\bf{x}}), k(\\bf{x}, \\bf{x}'))$.\n"," > ## A common example of a covariance function is the squared exponential $k({\\bf x}, {\\bf x}') = e^{-\\frac{\\|{\\bf x} - {\\bf x}^{'}\\|^2}{l^2}}$. The value of $l$ determines how fast the covariance between two points dies down as distance between them increases."]},{"cell_type":"markdown","metadata":{},"source":[" ## *The Gaussian process can be viewed as a probability distribution over functions*.\n"," > ## If we assume a Gaussian distribution, the we can sample points from that distribution. Similarly, if we assume a Gaussian process, then we can sample functions which satisfy the mean and covariance constraints."]},{"cell_type":"markdown","metadata":{},"source":[" ## Example:\n"," > ## Assume a Gaussian Process over the interval $[0,1]$, $f(x) \\sim \\mathcal{GP}(0, k(x, x'))$ where $k(x,x')$ is the squared exponential (RBF) kernel described previously. We now look at how sample functions can be created:\n"," >> ## Assume a fixed no.of samples $N$ (say $N=1000$) which is equally distributed over the interval $[0,1]$. Also fix a value for $l$.\n"," >> ## Then generate a sample from the multivariate normal distribution for ${\\bf x} = [x_1, x_2, \\dots, x_N]$ given by $f_{\\bf x} \\sim \\mathcal{N}({\\bf 0}, K({\\bf x}, {\\bf x}))$, where $K({\\bf x}, {\\bf x})$ is given by\n"," >> ## $K({\\bf x}, {\\bf x}) = \\left[ \\begin{array}{cccc} k(x_1, x_1) & k(x_1, x_2) & \\dots & k(x_1, x_N) \\\\ k(x_2, x_1) & k(x_2, x_2) & \\dots & k(x_2, x_N) \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ k(x_N, x_1) & k(x_N, x_2) & \\dots & k(x_N, x_N) \\\\ \\end{array} \\right]$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","N = 1000  # no.of sample points\n","l = 1  # std devation of covariance kernel\n","\n","# get the points in the interval (0,1)\n","x = np.linspace(0,1,N)\n","\n","# get the covariance matrix\n","K = np.zeros([N,N])\n","for i in range(N):\n","     for j in range(N):\n","          K[i,j] = np.exp(-(x[i]-x[j])*(x[i]-x[j])/(l*l))\n","\n","# generate samples from the multivariate normal distribution\n","mean = np.zeros(N)\n","fx1 = rand.multivariate_normal(mean, K)\n","fx2 = rand.multivariate_normal(mean, K)\n","fx3 = rand.multivariate_normal(mean, K)\n","\n","# plot the sample functions\n","t1 = go.Scatter(x=x, y=fx1, name='fx1',mode='markers')\n","t2 = go.Scatter(x=x, y=fx2, name='fx2',mode='markers')\n","t3 = go.Scatter(x=x, y=fx3, name='fx3',mode='markers')\n","fig3 = go.Figure(data=[t1, t2, t3])\n","fig3.update_layout(\n","     title = 'Sample functions generated for l=1',\n","     width=800, \n","     height=400,\n","     xaxis_title='x',\n","     yaxis_title='f(x)',\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","N = 1000  # no.of sample points\n","l = 0.1  # std devation of covariance kernel\n","\n","# get the points in the interval (0,1)\n","x = np.linspace(0,1,N)\n","\n","# get the covariance matrix\n","K = np.zeros([N,N])\n","for i in range(N):\n","     for j in range(N):\n","          K[i,j] = np.exp(-(x[i]-x[j])*(x[i]-x[j])/(l*l))\n","\n","# generate samples from the multivariate normal distribution\n","mean = np.zeros(N)\n","fx1 = rand.multivariate_normal(mean, K)\n","fx2 = rand.multivariate_normal(mean, K)\n","fx3 = rand.multivariate_normal(mean, K)\n","\n","# plot the sample functions\n","t1 = go.Scatter(x=x, y=fx1, name='fx1',mode='markers')\n","t2 = go.Scatter(x=x, y=fx2, name='fx2',mode='markers')\n","t3 = go.Scatter(x=x, y=fx3, name='fx3',mode='markers')\n","fig3 = go.Figure(data=[t1, t2, t3])\n","fig3.update_layout(\n","     title = 'Sample functions generated for l=0.1',\n","     width=800, \n","     height=400,\n","     xaxis_title='x',\n","     yaxis_title='f(x)',\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Gaussian Process Regression (GPR)\n"," ## The idea of GPR is: *Given a prior Gaussian process and a training dataset $(X,{\\bf y})$, arrive at a posterior Gaussian process which fits the data well.*\n"," ## Let $(X,{\\bf y})$ denote the training dataset with $X = [{\\bf x}_1, {\\bf x}_2, \\dots, {\\bf x}_N]$ and ${\\bf y} = [y_1, y_2, \\dots, y_N]$ denoting the function values. We want to be able to predict the function values ${\\bf y}_*$ at some other points $X_*$ different from the training points.\n"," ## Prior will be assumed to be $f({\\bf x}) \\sim \\mathcal{GP}({\\bf 0}, k({\\bf x}, {\\bf x'}))$.\n"," ## The Posterior in terms of values at test points is now given by\n"," ---\n"," > ## ${\\bf y}_* \\mid {\\bf y},X,X_* \\sim \\mathcal{N}({\\bf m}_*, \\text{cov}({\\bf y}_*))$,\n"," ---\n"," > ## ${\\bf m}_* = K(X_*,X) K(X,X)^{-1} {\\bf y}$,\n"," > ## $\\text{cov}({\\bf y}_*) = K(X_*,X_*)-K(X_*,X)K(X,X)^{-1}K(X,X_*)$."]},{"cell_type":"markdown","metadata":{},"source":[" # GPR with noisy observations\n"," ## In case of noisy observations\n"," > ## $y = f({\\bf x}) + \\epsilon$, $\\qquad$ where $\\epsilon \\sim \\mathcal{N}(0,\\sigma_n^2)$,\n"," ## the posterior is given by\n"," ---\n"," > ## ${\\bf y}_* \\mid {\\bf y},X,X_* \\sim \\mathcal{N}({\\bf m}_*, \\text{cov}({\\bf y}_*))$,\n"," ---\n"," > ## ${\\bf m}_* = K(X_*,X) \\left[ K(X,X) + \\sigma_n^2 I \\right]^{-1} {\\bf y}$,\n"," > ## $\\text{cov}({\\bf y}_*) = K(X_*,X_*)-K(X_*,X) \\left[ K(X,X) + \\sigma_n^2 I \\right]^{-1} K(X,X_*)$."]},{"cell_type":"markdown","metadata":{},"source":[" # A practical algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import Image\n","Image(filename='algo.jpg',width=1000, height=600)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Example"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# loading the dataset\n","filename = 'GPR with scikitlearn/Dataset 1'\n","input_df = pd.read_csv(r'{}.csv'.format(filename), low_memory=False)\n","\n","input_df = input_df.drop(columns=['Unnamed: 0'])\n","input_df\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","X = np.vstack(input_df['x'])\n","y = np.vstack(input_df['y'])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#plt.scatter(x=X, y=y)\n","plt.plot(y, marker='o', linestyle='')\n","plt.title('Raw data', fontsize=24)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Noise-free GPR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# length of data\n","N = X.shape[0]\n","\n","# parameter values to be used for l of the squared exponential kernel\n","l_list = np.array([1, 0.5])\n","\n","for i in range(len(l_list)):\n","\n","     # parameter l of the squared exponential kernel\n","     l = l_list[i]\n","     #l=1\n","     # training K\n","     K = np.zeros([N,N])\n","     for i in range(N):\n","          for j in range(N):\n","               K[i,j] = np.exp(-(X[i]-X[j])*(X[i]-X[j])/(l*l))\n","     Kinv = np.linalg.inv(K)\n","\n","     # test points\n","     Nt = 2000\n","     Xt = np.linspace(0,100,Nt)  # 2000 pts evenly distributed b/w 0 and 100\n","\n","     Ktt = np.zeros([Nt,Nt])\n","     for i in range(Nt):\n","          for j in range(Nt):\n","               Ktt[i,j] = np.exp(-(Xt[i]-Xt[j])*(Xt[i]-Xt[j])/(l*l))\n","\n","     Kt = np.zeros([Nt,N])\n","     for i in range(Nt):\n","          for j in range(N):\n","               Kt[i,j] = np.exp(-(Xt[i]-X[j])*(Xt[i]-X[j])/(l*l))\n","\n","     # mean of test set\n","     m = np.dot(Kt, np.dot(Kinv,y))\n","     # covariance of test set\n","     cov = Ktt - np.dot(Kt, np.dot(Kinv, Kt.transpose()))\n","     std = np.diag(np.sqrt(cov))\n","     conf1 = m.flatten() - std\n","     conf2 = m.flatten() + std\n","\n","     # plot the sample functions\n","     t = []\n","     t.append(go.Scatter(x=Xt, y=conf1, name='lower conf. bound', mode='lines', line_color='grey'))\n","     t.append(go.Scatter(x=Xt, y=conf2, name='upper conf. bound', mode='lines', fill='tonexty', line_color='grey'))\n","     t.append(go.Scatter(x=Xt, y=m.flatten(), name='mean', mode='markers', marker_size=3))\n","     t.append(go.Scatter(x=X.flatten(), y=y.flatten(), name='raw data points', mode='markers', marker_size=3, marker_color='red'))\n","\n","     fig4 = go.Figure(data=t)\n","     fig4.update_layout(\n","          title = 'Predictions for l='+str(l),\n","          width=800, \n","          height=400,\n","          xaxis_title='x',\n","          yaxis_title='f(x)',\n","     )\n","     fig4.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # GPR with noise"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# length of data\n","N = X.shape[0]\n","\n","# parameter values to be used for l of the squared exponential kernel - (l,sigma_n)\n","l_list = np.array([(2, 0.7), (1, 0.7), (0.5, 0.7), (2, 0.3), (1, 0.3), (0.5, 0.3)])\n","\n","for i in range(len(l_list)):\n","\n","     # parameter l of the squared exponential kernel\n","     l = l_list[i][0]\n","     s = l_list[i][1]  # noise variance\n","     #l=1\n","     # training K\n","     K = np.zeros([N,N])\n","     for i in range(N):\n","          for j in range(N):\n","               K[i,j] = np.exp(-(X[i]-X[j])*(X[i]-X[j])/(l*l))\n","     K_ = K + s*s*np.eye(N)\n","     Kinv = np.linalg.inv(K_)\n","\n","     # test points\n","     Nt = 2000\n","     Xt = np.linspace(0,100,Nt)  # 2000 pts evenly distributed b/w 0 and 100\n","\n","     Ktt = np.zeros([Nt,Nt])\n","     for i in range(Nt):\n","          for j in range(Nt):\n","               Ktt[i,j] = np.exp(-(Xt[i]-Xt[j])*(Xt[i]-Xt[j])/(l*l))\n","\n","     Kt = np.zeros([Nt,N])\n","     for i in range(Nt):\n","          for j in range(N):\n","               Kt[i,j] = np.exp(-(Xt[i]-X[j])*(Xt[i]-X[j])/(l*l))\n","\n","     # mean of test set\n","     m = np.dot(Kt, np.dot(Kinv,y))\n","     # covariance of test set\n","     cov = Ktt - np.dot(Kt, np.dot(Kinv, Kt.transpose()))\n","     std = np.diag(np.sqrt(cov))\n","     conf1 = m.flatten() - std\n","     conf2 = m.flatten() + std\n","\n","     # plot the sample functions\n","     t = []\n","     t.append(go.Scatter(x=Xt, y=conf1, name='lower conf. bound', mode='lines', line_color='grey'))\n","     t.append(go.Scatter(x=Xt, y=conf2, name='upper conf. bound', mode='lines', fill='tonexty', line_color='grey'))\n","     t.append(go.Scatter(x=Xt, y=m.flatten(), name='mean', mode='markers', marker_size=3))\n","     t.append(go.Scatter(x=X.flatten(), y=y.flatten(), name='raw data points', mode='markers', marker_size=3, marker_color='red'))\n","\n","     fig5 = go.Figure(data=t)\n","     fig5.update_layout(\n","          title = 'Predictions for s='+str(s)+', l='+str(l),\n","          width=800, \n","          height=400,\n","          xaxis_title='x',\n","          yaxis_title='f(x)',\n","     )\n","     fig5.show()\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}